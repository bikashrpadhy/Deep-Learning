{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bikashrpadhy/Deep-Learning/blob/main/DL_Torch_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVr-nNcULt0z"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.__version__"
      ],
      "metadata": {
        "id": "2Ln9mgPnL9l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/tensors.html"
      ],
      "metadata": {
        "id": "HWOYbkc9MR6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([[1,2],[3,4]],dtype=torch.float)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0qUK0WfMBHE",
        "outputId": "0fe525ef-4736-4279-9c44-e610d510b9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7C2O9e2N47c",
        "outputId": "e8cb5cc6-acc5-4369-888f-8ac07e0ba437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.nelement()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEbpyhv3NBSV",
        "outputId": "b4630d46-4a39-4467-eb1b-56ecc2c26433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix=torch.tensor([[1,2,3],[3,4,5],[5,6,7],[7,8,9]])\n"
      ],
      "metadata": {
        "id": "22eJw_N8OaTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjYNkaMUOblc",
        "outputId": "4dcd28b0-6120-4612-b35b-cdb75ced9b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix.nelement()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgULRHT_OgWj",
        "outputId": "4501f90b-ebd8-4528-9380-435b6be3d212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z=torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])"
      ],
      "metadata": {
        "id": "rQo0H9CSPOL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9kUE8b6TO7L",
        "outputId": "10dc6d7e-e06c-45d2-f953-56a4a933584a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z1= z.view(-1, 4)\n",
        "z1.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_YzXz_xr-Nc",
        "outputId": "47ccfa53-ec5a-4e8c-dd8f-75a7d5ec302f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.transpose(0,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD72THVFTSSp",
        "outputId": "3fdff552-31ca-4a9c-bca6-9f9c0ebbb6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 5],\n",
              "         [3, 7]],\n",
              "\n",
              "        [[2, 6],\n",
              "         [4, 8]]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"input=torch.randn((100,3,28,28))\n",
        "ground_truth = torch.ones(100).to(torch.long)\"\"\""
      ],
      "metadata": {
        "id": "GKj_H_mLT-uv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a079224c-a778-42ad-c123-a9fdfd2f23d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'input=torch.randn((100,3,28,28))\\nground_truth = torch.ones(100).to(torch.long)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input.ndim"
      ],
      "metadata": {
        "id": "Grb-k2eCUIVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "7Br-fuBYV-uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNet(nn.Module):\n",
        "  def __init__(self,input_size= 1*28*28, output_size=10):\n",
        "    super().__init__()\n",
        "    self.input_size=input_size\n",
        "    self.output_size=output_size\n",
        "    self.fc1 = nn.Linear(self.input_size,64)\n",
        "    self.fc2 = nn.Linear(64,128)\n",
        "    # self.fc3 = nn.Linear(128,64)\n",
        "    # self.fc4 = nn.Linear(64, 32)\n",
        "    self.output = nn.Linear(128,self.output_size)\n",
        "\n",
        "  def forward(self,x):# input is 64 x 1 x 28 x 28\n",
        "    x= x.view(-1,self.input_size)# x is 0x3*784 ; -1 means we want the computer to figure out the rest of the dimension by itself\n",
        "    # x=self.fc1(x) #self.fc1.forward(x)\n",
        "    x = F.sigmoid(self.fc1(x))\n",
        "    x = F.sigmoid(self.fc2(x))\n",
        "    # x = F.sigmoid(self.fc3(x))\n",
        "    # x = F.sigmoid(self.fc4(x))\n",
        "    x = F.sigmoid(self.output(x))\n",
        "\n",
        "    return x # x is 100 x 10\n",
        "net = MyNet()\n",
        "# output = net(images)\n",
        "# print(net.fc1.weight.shape,net.fc1.bias.shape)"
      ],
      "metadata": {
        "id": "su8SYO5JWC21"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# def criterion(y_pred, y):\n",
        "#     out = -1 * torch.mean(y * torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred))\n",
        "#     return out\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr = 0.1)"
      ],
      "metadata": {
        "id": "Y8bjenevg6SJ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# for epoch in range(5):\n",
        "#   output = net(input)\n",
        "#   optimizer.zero_grad()  #____\n",
        "#   loss = criterion(output, ground_truth)  #rank with 0 tensor\n",
        "#   loss.backward() #gradient of loss wrt all weights and biases computed and stored\n",
        "#   print(net.fc1.weight.requires_grad,net.fc1.weight.grad)\n",
        "#   # assert False\n",
        "#   optimizer.step()  #updates weights and biases\n",
        "#   print(output.shape)"
      ],
      "metadata": {
        "id": "OOrTIrUlXf7D"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "root='/content/drive/MyDrive/Colab Notebooks/pytorchtut'"
      ],
      "metadata": {
        "id": "2IJizDQr2A0J"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transform.normalize see about this in docs\n",
        " pytorch.org/vision/stable/transforms.html\n"
      ],
      "metadata": {
        "id": "EA2FAMoI3e7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from torchvision.transforms.transforms import ToTensor\n",
        "import torchvision\n",
        "from torchvision import transforms as transforms\n",
        "train_transforms=transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                               ])\n",
        "\n",
        "# train_mnist_dataset=torchvision.datasets.MNIST(root=root, train=True, download=True,\n",
        "#                              transform=torchvision.transforms.Compose(\n",
        "#                                 [\n",
        "#                                torchvision.transforms.ToTensor(),\n",
        "#                                torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                                ] ))\n",
        "\n",
        "train_mnist_dataset=torchvision.datasets.MNIST(root=root,train=True,download=True,transform=train_transforms)\n",
        "test_mnist_dataset=torchvision.datasets.MNIST(root=root,train=False,download=True,transform=train_transforms)"
      ],
      "metadata": {
        "id": "5s4qioB1tsOQ",
        "collapsed": true
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "type(train_mnist_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64VMFd5E423v",
        "outputId": "647694f6-f6b4-4b46-a61b-bde039bf40d5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.mnist.MNIST"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title\n",
        "# train_mnist_dataset[0]"
      ],
      "metadata": {
        "id": "wtmP16Ot5XS2",
        "collapsed": true
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#torch.utills.data.DataLoader()\n",
        "import torch.utils.data\n",
        "\n",
        "train_loader=torch.utils.data.DataLoader(train_mnist_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader=torch.utils.data.DataLoader( test_mnist_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "ZWXvoSYg6Kta"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title\n",
        "# # Lets visualise the data\n",
        "# examples = enumerate(test_loader)\n",
        "# batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig = plt.figure()\n",
        "# for i in range(6):\n",
        "#   plt.subplot(2,3,i+1)\n",
        "#   plt.tight_layout()\n",
        "#   plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "#   plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "#   plt.xticks([])\n",
        "#   plt.yticks([])\n",
        "# fig"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Uia-2ZCipZuD"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This code is written without taking GPU availabilty into consideration \"\"\"\n",
        "\n",
        "# # net = MyNet()\n",
        "# for epoch in range(5):\n",
        "#   epoch_loss = 0\n",
        "#   for i,data in enumerate(train_loader): # data is a tensor of 2 elements only: images and labels\n",
        "#     images=data[0] # A tensor\n",
        "#     labels=data[1] # a tensor\n",
        "#     # print(\"labels type\",type(labels))\n",
        "#     # print(labels[0]) # tensor(5)\n",
        "#     # print(images.shape,labels.shape)\n",
        "#     # assert False\n",
        "#     output = net(images)\n",
        "#     optimizer.zero_grad()  # make all the previous grads zero, else new grads will be sum of old and current grads\n",
        "#     loss = criterion(output, labels)  #rank with 0 tensor\n",
        "#     # loss = nn.CrossEntropyLoss()(output, labels)\n",
        "#     loss.backward() #gradient of loss wrt all weights and biases computed and stored\n",
        "#     # print(net.fc1.weight.requires_grad,net.fc1.weight.grad)\n",
        "#     # assert False\n",
        "#     optimizer.step()  #updates weights and biases\n",
        "#     epoch_loss += loss.item()  #Returns the value of this tensor as a standard Python number. This only works\n",
        "#   print(f\"Epoch {epoch}/5,loss = {epoch_loss/len(train_loader)} \")"
      ],
      "metadata": {
        "id": "HuuZnQlj8GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "20659100-e711-45f3-9038-3bc59c82d0d7"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' This code is written without taking GPU availabilty into consideration '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transfer the training and testng to GPU if it's available"
      ],
      "metadata": {
        "id": "tIjyImQNmZNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "r1HYQgwv_jeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13786d95-b5fa-4276-ce7f-1706ec352f3a"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net=net.to(device)\n",
        "criterion=criterion.to(device)"
      ],
      "metadata": {
        "id": "xNN3WSro_2Rb"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(90):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc=0\n",
        "  for i,data in enumerate(train_loader):\n",
        "    images=data[0]\n",
        "    labels=data[1]\n",
        "    images=images.to(device)\n",
        "    labels=labels.to(device)\n",
        "    # print(images.shape,labels.shape)\n",
        "    # assert False\n",
        "    output = net(images)\n",
        "    optimizer.zero_grad()  #____\n",
        "    loss = criterion(output, labels)  #rank with 0 tensor\n",
        "    loss.backward() #gradient of loss wrt all weights and biases computed and stored\n",
        "    # print(net.fc1.weight.requires_grad,net.fc1.weight.grad)\n",
        "    # assert False\n",
        "    optimizer.step()  #updates weights and biases\n",
        "    epoch_loss += loss.item()  #Returns the value of this tensor as a standard Python number. This only works\n",
        "\n",
        "    pred= torch.argmax(output, dim=1)\n",
        "    # print(\"pred:\", pred)\n",
        "    # print(\"pred.shape:\", pred.shape)\n",
        "    # assert False\n",
        "    acc = (pred == labels).float().mean()\n",
        "    epoch_acc += acc\n",
        "    # print(\"acc: \", acc)\n",
        "    # print(f\"pred shape: {pred.shape}\")\n",
        "    # assert False\n",
        "  print(f\"Epoch {epoch}, loss = {epoch_loss/len(train_loader)} \")\n",
        "  print(f\"Epoch {epoch}, acc % = {epoch_acc*100/len(train_loader)} \")\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "# Epoch 5, acc = 0.7414546012878418 // layers= only 2, lr= 0.01\n",
        "# Epoch 49, acc % = 98.16098022460938\n",
        "# Epoch 61, acc % = 98.30757141113281 //  layers= only 2, lr= 0.1\n",
        "# Epoch 89, acc % = 98.5307846069336  layers= only 2, lr= 0.1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ynFoJ0PxATsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696b0d41-edb9-4315-f165-9b38c0b22ae0"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss = 1.5040988061728 \n",
            "Epoch 0, acc % = 94.29637908935547 \n",
            "\n",
            "Epoch 1, loss = 1.5013835752950802 \n",
            "Epoch 1, acc % = 95.07762908935547 \n",
            "\n",
            "Epoch 2, loss = 1.4996845780659331 \n",
            "Epoch 2, acc % = 95.65731811523438 \n",
            "\n",
            "Epoch 3, loss = 1.4979995487849596 \n",
            "Epoch 3, acc % = 96.20369720458984 \n",
            "\n",
            "Epoch 4, loss = 1.4962628533336908 \n",
            "Epoch 4, acc % = 96.62346649169922 \n",
            "\n",
            "Epoch 5, loss = 1.4946525692939758 \n",
            "Epoch 5, acc % = 96.86833953857422 \n",
            "\n",
            "Epoch 6, loss = 1.493338588839655 \n",
            "Epoch 6, acc % = 97.02992248535156 \n",
            "\n",
            "Epoch 7, loss = 1.4923251370377124 \n",
            "Epoch 7, acc % = 97.13652801513672 \n",
            "\n",
            "Epoch 8, loss = 1.4915367851633508 \n",
            "Epoch 8, acc % = 97.17650604248047 \n",
            "\n",
            "Epoch 9, loss = 1.4908479750791848 \n",
            "Epoch 9, acc % = 97.25146484375 \n",
            "\n",
            "Epoch 10, loss = 1.4902455938904524 \n",
            "Epoch 10, acc % = 97.27645111083984 \n",
            "\n",
            "Epoch 11, loss = 1.4897313025206136 \n",
            "Epoch 11, acc % = 97.36473846435547 \n",
            "\n",
            "Epoch 12, loss = 1.4892813877256186 \n",
            "Epoch 12, acc % = 97.3880615234375 \n",
            "\n",
            "Epoch 13, loss = 1.4888473750431654 \n",
            "Epoch 13, acc % = 97.40805053710938 \n",
            "\n",
            "Epoch 14, loss = 1.4884065049035209 \n",
            "Epoch 14, acc % = 97.4530258178711 \n",
            "\n",
            "Epoch 15, loss = 1.4880297571611303 \n",
            "Epoch 15, acc % = 97.47301483154297 \n",
            "\n",
            "Epoch 16, loss = 1.4876820170548932 \n",
            "Epoch 16, acc % = 97.52965545654297 \n",
            "\n",
            "Epoch 17, loss = 1.4873191918899764 \n",
            "Epoch 17, acc % = 97.54631042480469 \n",
            "\n",
            "Epoch 18, loss = 1.4869915049975868 \n",
            "Epoch 18, acc % = 97.56629943847656 \n",
            "\n",
            "Epoch 19, loss = 1.486661514112436 \n",
            "Epoch 19, acc % = 97.60128021240234 \n",
            "\n",
            "Epoch 20, loss = 1.4863751624692987 \n",
            "Epoch 20, acc % = 97.6412582397461 \n",
            "\n",
            "Epoch 21, loss = 1.4861020979596609 \n",
            "Epoch 21, acc % = 97.65458679199219 \n",
            "\n",
            "Epoch 22, loss = 1.4858438390404431 \n",
            "Epoch 22, acc % = 97.67790985107422 \n",
            "\n",
            "Epoch 23, loss = 1.4855353379808764 \n",
            "Epoch 23, acc % = 97.70955657958984 \n",
            "\n",
            "Epoch 24, loss = 1.4852556755293662 \n",
            "Epoch 24, acc % = 97.7278823852539 \n",
            "\n",
            "Epoch 25, loss = 1.4850016204533039 \n",
            "Epoch 25, acc % = 97.74787139892578 \n",
            "\n",
            "Epoch 26, loss = 1.4847521773025172 \n",
            "Epoch 26, acc % = 97.76952362060547 \n",
            "\n",
            "Epoch 27, loss = 1.4845327121108325 \n",
            "Epoch 27, acc % = 97.78784942626953 \n",
            "\n",
            "Epoch 28, loss = 1.484296844458021 \n",
            "Epoch 28, acc % = 97.82283020019531 \n",
            "\n",
            "Epoch 29, loss = 1.4840872348752865 \n",
            "Epoch 29, acc % = 97.84615325927734 \n",
            "\n",
            "Epoch 30, loss = 1.4838512980861704 \n",
            "Epoch 30, acc % = 97.85281372070312 \n",
            "\n",
            "Epoch 31, loss = 1.483654696041587 \n",
            "Epoch 31, acc % = 97.87113952636719 \n",
            "\n",
            "Epoch 32, loss = 1.48345216581308 \n",
            "Epoch 32, acc % = 97.89279174804688 \n",
            "\n",
            "Epoch 33, loss = 1.4832556337944225 \n",
            "Epoch 33, acc % = 97.92277526855469 \n",
            "\n",
            "Epoch 34, loss = 1.4830479696869596 \n",
            "Epoch 34, acc % = 97.94276428222656 \n",
            "\n",
            "Epoch 35, loss = 1.482874999168331 \n",
            "Epoch 35, acc % = 97.95942687988281 \n",
            "\n",
            "Epoch 36, loss = 1.482641909803663 \n",
            "Epoch 36, acc % = 97.94276428222656 \n",
            "\n",
            "Epoch 37, loss = 1.4824912040980893 \n",
            "Epoch 37, acc % = 97.97274780273438 \n",
            "\n",
            "Epoch 38, loss = 1.4822874843184628 \n",
            "Epoch 38, acc % = 97.9960708618164 \n",
            "\n",
            "Epoch 39, loss = 1.482094773732777 \n",
            "Epoch 39, acc % = 98.01439666748047 \n",
            "\n",
            "Epoch 40, loss = 1.481906187686839 \n",
            "Epoch 40, acc % = 98.03271484375 \n",
            "\n",
            "Epoch 41, loss = 1.48174421276365 \n",
            "Epoch 41, acc % = 98.03438568115234 \n",
            "\n",
            "Epoch 42, loss = 1.4815663977472513 \n",
            "Epoch 42, acc % = 98.06436920166016 \n",
            "\n",
            "Epoch 43, loss = 1.481409614020065 \n",
            "Epoch 43, acc % = 98.0743637084961 \n",
            "\n",
            "Epoch 44, loss = 1.481229577237355 \n",
            "Epoch 44, acc % = 98.09435272216797 \n",
            "\n",
            "Epoch 45, loss = 1.4810653133178824 \n",
            "Epoch 45, acc % = 98.09768676757812 \n",
            "\n",
            "Epoch 46, loss = 1.480923892211304 \n",
            "Epoch 46, acc % = 98.1043472290039 \n",
            "\n",
            "Epoch 47, loss = 1.480743509747072 \n",
            "Epoch 47, acc % = 98.13932800292969 \n",
            "\n",
            "Epoch 48, loss = 1.4806344670527525 \n",
            "Epoch 48, acc % = 98.1259994506836 \n",
            "\n",
            "Epoch 49, loss = 1.480467827716616 \n",
            "Epoch 49, acc % = 98.16098022460938 \n",
            "\n",
            "Epoch 50, loss = 1.4803114422857127 \n",
            "Epoch 50, acc % = 98.17264556884766 \n",
            "\n",
            "Epoch 51, loss = 1.4801549187092893 \n",
            "Epoch 51, acc % = 98.17764282226562 \n",
            "\n",
            "Epoch 52, loss = 1.4800475143166238 \n",
            "Epoch 52, acc % = 98.18596649169922 \n",
            "\n",
            "Epoch 53, loss = 1.4798869946872248 \n",
            "Epoch 53, acc % = 98.20928955078125 \n",
            "\n",
            "Epoch 54, loss = 1.4797522109200452 \n",
            "Epoch 54, acc % = 98.24594116210938 \n",
            "\n",
            "Epoch 55, loss = 1.4795946058179779 \n",
            "Epoch 55, acc % = 98.22261810302734 \n",
            "\n",
            "Epoch 56, loss = 1.4794737047223903 \n",
            "Epoch 56, acc % = 98.25593566894531 \n",
            "\n",
            "Epoch 57, loss = 1.4793625435849498 \n",
            "Epoch 57, acc % = 98.25093841552734 \n",
            "\n",
            "Epoch 58, loss = 1.479221141795868 \n",
            "Epoch 58, acc % = 98.27425384521484 \n",
            "\n",
            "Epoch 59, loss = 1.479112819822104 \n",
            "Epoch 59, acc % = 98.28591918945312 \n",
            "\n",
            "Epoch 60, loss = 1.4789722474144975 \n",
            "Epoch 60, acc % = 98.30257415771484 \n",
            "\n",
            "Epoch 61, loss = 1.478860447020419 \n",
            "Epoch 61, acc % = 98.30757141113281 \n",
            "\n",
            "Epoch 62, loss = 1.478740731027843 \n",
            "Epoch 62, acc % = 98.30257415771484 \n",
            "\n",
            "Epoch 63, loss = 1.4786387193940087 \n",
            "Epoch 63, acc % = 98.31922912597656 \n",
            "\n",
            "Epoch 64, loss = 1.4785211506937104 \n",
            "Epoch 64, acc % = 98.35088348388672 \n",
            "\n",
            "Epoch 65, loss = 1.4783995225231277 \n",
            "Epoch 65, acc % = 98.33755493164062 \n",
            "\n",
            "Epoch 66, loss = 1.4782817638250811 \n",
            "Epoch 66, acc % = 98.36420440673828 \n",
            "\n",
            "Epoch 67, loss = 1.4781805495463454 \n",
            "Epoch 67, acc % = 98.36254119873047 \n",
            "\n",
            "Epoch 68, loss = 1.4780784764015344 \n",
            "Epoch 68, acc % = 98.3725357055664 \n",
            "\n",
            "Epoch 69, loss = 1.4779699809515654 \n",
            "Epoch 69, acc % = 98.38420104980469 \n",
            "\n",
            "Epoch 70, loss = 1.4778892930382606 \n",
            "Epoch 70, acc % = 98.37586975097656 \n",
            "\n",
            "Epoch 71, loss = 1.4777705754552568 \n",
            "Epoch 71, acc % = 98.39419555664062 \n",
            "\n",
            "Epoch 72, loss = 1.4776711237710167 \n",
            "Epoch 72, acc % = 98.41918182373047 \n",
            "\n",
            "Epoch 73, loss = 1.4775718894086158 \n",
            "Epoch 73, acc % = 98.43083953857422 \n",
            "\n",
            "Epoch 74, loss = 1.4774793359770704 \n",
            "Epoch 74, acc % = 98.40918731689453 \n",
            "\n",
            "Epoch 75, loss = 1.4773830096604728 \n",
            "Epoch 75, acc % = 98.44416809082031 \n",
            "\n",
            "Epoch 76, loss = 1.4773006005836193 \n",
            "Epoch 76, acc % = 98.44916534423828 \n",
            "\n",
            "Epoch 77, loss = 1.477207214593379 \n",
            "Epoch 77, acc % = 98.4658203125 \n",
            "\n",
            "Epoch 78, loss = 1.4771172316597978 \n",
            "Epoch 78, acc % = 98.47081756591797 \n",
            "\n",
            "Epoch 79, loss = 1.4770265601591261 \n",
            "Epoch 79, acc % = 98.48247528076172 \n",
            "\n",
            "Epoch 80, loss = 1.4769355484417506 \n",
            "Epoch 80, acc % = 98.4791488647461 \n",
            "\n",
            "Epoch 81, loss = 1.4768687448521922 \n",
            "Epoch 81, acc % = 98.49246978759766 \n",
            "\n",
            "Epoch 82, loss = 1.4767680867140227 \n",
            "Epoch 82, acc % = 98.49580383300781 \n",
            "\n",
            "Epoch 83, loss = 1.4767092288430057 \n",
            "Epoch 83, acc % = 98.49913787841797 \n",
            "\n",
            "Epoch 84, loss = 1.4766159052533636 \n",
            "Epoch 84, acc % = 98.51412963867188 \n",
            "\n",
            "Epoch 85, loss = 1.476525633192774 \n",
            "Epoch 85, acc % = 98.52412414550781 \n",
            "\n",
            "Epoch 86, loss = 1.47644861496842 \n",
            "Epoch 86, acc % = 98.52745056152344 \n",
            "\n",
            "Epoch 87, loss = 1.476398261244109 \n",
            "Epoch 87, acc % = 98.53578186035156 \n",
            "\n",
            "Epoch 88, loss = 1.4762826541593588 \n",
            "Epoch 88, acc % = 98.53411865234375 \n",
            "\n",
            "Epoch 89, loss = 1.4762227879658436 \n",
            "Epoch 89, acc % = 98.5307846069336 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" predict the classes for the test images\"\"\"\n",
        "\n",
        "for epoch in range(90):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc =0\n",
        "  for i,data in enumerate(test_loader):\n",
        "    images=data[0]\n",
        "    labels=data[1]\n",
        "    images=images.to(device)\n",
        "    labels=labels.to(device)\n",
        "    # print(\"Images, labels shapes: \", images.shape,labels.shape)\n",
        "    # Images, labels shapes:  torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
        "    # assert False\n",
        "    output = net(images)\n",
        "    optimizer.zero_grad()  #____\n",
        "    loss = criterion(output, labels)  #rank with 0 tensor\n",
        "    loss.backward() #gradient of loss wrt all weights and biases computed and stored\n",
        "    # print(net.fc1.weight.requires_grad,net.fc1.weight.grad)\n",
        "    # assert False\n",
        "    optimizer.step()  #updates weights and biases\n",
        "    epoch_loss += loss.item()  #Returns the value of this tensor as a standard Python number. This only works\n",
        "\n",
        "    # print(\"output[0]: \", output[0])\n",
        "    pred= torch.argmax(output, dim=1)\n",
        "    # print(\"pred:\", pred)\n",
        "    # print(\"pred.shape:\", pred.shape)\n",
        "    # assert False\n",
        "    acc = (pred == labels).float().mean()\n",
        "    epoch_acc += acc\n",
        "    # print(\"acc: \", acc)\n",
        "    # print(f\"pred shape: {pred.shape}\")\n",
        "    # assert False\n",
        "  print(f\"Epoch {epoch}, loss = {epoch_loss/len(test_loader)} \")\n",
        "  print(f\"Epoch {epoch}, acc % = {epoch_acc*100/len(test_loader)} \")\n",
        "  print(\"\")\n",
        "   # evaluate model at end of epoch\n",
        "\n",
        "\n",
        "# Epoch 59/5, loss = 1.4898245835759838\n",
        "# Epoch 59/5, acc % = 95.26274108886719\n",
        "# Epoch 89, acc % = 99.11425018310547\n",
        "# testigng was much faster than training\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P13OX34fANBS",
        "outputId": "2bf0f9c5-3729-42c3-bd7f-be65456f1d8d",
        "collapsed": true
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss = 1.4879392271588563 \n",
            "Epoch 0, acc % = 97.50199127197266 \n",
            "\n",
            "Epoch 1, loss = 1.485174410662074 \n",
            "Epoch 1, acc % = 97.69108581542969 \n",
            "\n",
            "Epoch 2, loss = 1.4836435773570067 \n",
            "Epoch 2, acc % = 97.75080108642578 \n",
            "\n",
            "Epoch 3, loss = 1.482550662034636 \n",
            "Epoch 3, acc % = 97.93988800048828 \n",
            "\n",
            "Epoch 4, loss = 1.4816900233554233 \n",
            "Epoch 4, acc % = 98.00955200195312 \n",
            "\n",
            "Epoch 5, loss = 1.4809742854658965 \n",
            "Epoch 5, acc % = 98.06926727294922 \n",
            "\n",
            "Epoch 6, loss = 1.4803370973866457 \n",
            "Epoch 6, acc % = 98.1886978149414 \n",
            "\n",
            "Epoch 7, loss = 1.4797630978238052 \n",
            "Epoch 7, acc % = 98.22850799560547 \n",
            "\n",
            "Epoch 8, loss = 1.4792573102720223 \n",
            "Epoch 8, acc % = 98.25836181640625 \n",
            "\n",
            "Epoch 9, loss = 1.4788081547257248 \n",
            "Epoch 9, acc % = 98.28821563720703 \n",
            "\n",
            "Epoch 10, loss = 1.4784014627432367 \n",
            "Epoch 10, acc % = 98.30812072753906 \n",
            "\n",
            "Epoch 11, loss = 1.478041976880116 \n",
            "Epoch 11, acc % = 98.3280258178711 \n",
            "\n",
            "Epoch 12, loss = 1.4777266022506033 \n",
            "Epoch 12, acc % = 98.40764617919922 \n",
            "\n",
            "Epoch 13, loss = 1.4774439107080934 \n",
            "Epoch 13, acc % = 98.42755126953125 \n",
            "\n",
            "Epoch 14, loss = 1.4771844619398664 \n",
            "Epoch 14, acc % = 98.42755126953125 \n",
            "\n",
            "Epoch 15, loss = 1.476941794346852 \n",
            "Epoch 15, acc % = 98.45740509033203 \n",
            "\n",
            "Epoch 16, loss = 1.4767135070387725 \n",
            "Epoch 16, acc % = 98.48725891113281 \n",
            "\n",
            "Epoch 17, loss = 1.4764986379890686 \n",
            "Epoch 17, acc % = 98.51712036132812 \n",
            "\n",
            "Epoch 18, loss = 1.476293879709426 \n",
            "Epoch 18, acc % = 98.52706909179688 \n",
            "\n",
            "Epoch 19, loss = 1.47609590724775 \n",
            "Epoch 19, acc % = 98.53702545166016 \n",
            "\n",
            "Epoch 20, loss = 1.475904121520413 \n",
            "Epoch 20, acc % = 98.58678436279297 \n",
            "\n",
            "Epoch 21, loss = 1.4757218209041911 \n",
            "Epoch 21, acc % = 98.61663818359375 \n",
            "\n",
            "Epoch 22, loss = 1.47555160218743 \n",
            "Epoch 22, acc % = 98.62659454345703 \n",
            "\n",
            "Epoch 23, loss = 1.4753913287144558 \n",
            "Epoch 23, acc % = 98.62659454345703 \n",
            "\n",
            "Epoch 24, loss = 1.4752378623197033 \n",
            "Epoch 24, acc % = 98.64649963378906 \n",
            "\n",
            "Epoch 25, loss = 1.475088798316421 \n",
            "Epoch 25, acc % = 98.67635345458984 \n",
            "\n",
            "Epoch 26, loss = 1.4749417517595231 \n",
            "Epoch 26, acc % = 98.69625854492188 \n",
            "\n",
            "Epoch 27, loss = 1.4747945746039128 \n",
            "Epoch 27, acc % = 98.7161636352539 \n",
            "\n",
            "Epoch 28, loss = 1.474650979801348 \n",
            "Epoch 28, acc % = 98.70621490478516 \n",
            "\n",
            "Epoch 29, loss = 1.474518825294106 \n",
            "Epoch 29, acc % = 98.70621490478516 \n",
            "\n",
            "Epoch 30, loss = 1.474396238661116 \n",
            "Epoch 30, acc % = 98.70621490478516 \n",
            "\n",
            "Epoch 31, loss = 1.4742801986682188 \n",
            "Epoch 31, acc % = 98.73606872558594 \n",
            "\n",
            "Epoch 32, loss = 1.4741691616690082 \n",
            "Epoch 32, acc % = 98.75597381591797 \n",
            "\n",
            "Epoch 33, loss = 1.4740622066388465 \n",
            "Epoch 33, acc % = 98.75597381591797 \n",
            "\n",
            "Epoch 34, loss = 1.4739582394338717 \n",
            "Epoch 34, acc % = 98.76592254638672 \n",
            "\n",
            "Epoch 35, loss = 1.4738559859573461 \n",
            "Epoch 35, acc % = 98.75597381591797 \n",
            "\n",
            "Epoch 36, loss = 1.4737541075724705 \n",
            "Epoch 36, acc % = 98.78582763671875 \n",
            "\n",
            "Epoch 37, loss = 1.4736521889449685 \n",
            "Epoch 37, acc % = 98.79578399658203 \n",
            "\n",
            "Epoch 38, loss = 1.473551297643382 \n",
            "Epoch 38, acc % = 98.81568908691406 \n",
            "\n",
            "Epoch 39, loss = 1.473453318996794 \n",
            "Epoch 39, acc % = 98.82563781738281 \n",
            "\n",
            "Epoch 40, loss = 1.47335955975162 \n",
            "Epoch 40, acc % = 98.82563781738281 \n",
            "\n",
            "Epoch 41, loss = 1.4732703190700265 \n",
            "Epoch 41, acc % = 98.8355941772461 \n",
            "\n",
            "Epoch 42, loss = 1.4731855855625906 \n",
            "Epoch 42, acc % = 98.84554290771484 \n",
            "\n",
            "Epoch 43, loss = 1.4731050942354142 \n",
            "Epoch 43, acc % = 98.86544799804688 \n",
            "\n",
            "Epoch 44, loss = 1.4730284456994123 \n",
            "Epoch 44, acc % = 98.86544799804688 \n",
            "\n",
            "Epoch 45, loss = 1.4729552299353728 \n",
            "Epoch 45, acc % = 98.86544799804688 \n",
            "\n",
            "Epoch 46, loss = 1.4728849488458815 \n",
            "Epoch 46, acc % = 98.8853530883789 \n",
            "\n",
            "Epoch 47, loss = 1.472817203801149 \n",
            "Epoch 47, acc % = 98.89530181884766 \n",
            "\n",
            "Epoch 48, loss = 1.4727515521322845 \n",
            "Epoch 48, acc % = 98.89530181884766 \n",
            "\n",
            "Epoch 49, loss = 1.4726876453229576 \n",
            "Epoch 49, acc % = 98.89530181884766 \n",
            "\n",
            "Epoch 50, loss = 1.4726251090408131 \n",
            "Epoch 50, acc % = 98.89530181884766 \n",
            "\n",
            "Epoch 51, loss = 1.472563483912474 \n",
            "Epoch 51, acc % = 98.89530181884766 \n",
            "\n",
            "Epoch 52, loss = 1.472502181484441 \n",
            "Epoch 52, acc % = 98.90525817871094 \n",
            "\n",
            "Epoch 53, loss = 1.4724405297807828 \n",
            "Epoch 53, acc % = 98.90525817871094 \n",
            "\n",
            "Epoch 54, loss = 1.4723774870489812 \n",
            "Epoch 54, acc % = 98.91520690917969 \n",
            "\n",
            "Epoch 55, loss = 1.4723120130551088 \n",
            "Epoch 55, acc % = 98.92516326904297 \n",
            "\n",
            "Epoch 56, loss = 1.4722435117527177 \n",
            "Epoch 56, acc % = 98.91520690917969 \n",
            "\n",
            "Epoch 57, loss = 1.472171383298886 \n",
            "Epoch 57, acc % = 98.92516326904297 \n",
            "\n",
            "Epoch 58, loss = 1.4720920840646052 \n",
            "Epoch 58, acc % = 98.93511199951172 \n",
            "\n",
            "Epoch 59, loss = 1.4720073634651816 \n",
            "Epoch 59, acc % = 98.945068359375 \n",
            "\n",
            "Epoch 60, loss = 1.4719294696856455 \n",
            "Epoch 60, acc % = 98.95501708984375 \n",
            "\n",
            "Epoch 61, loss = 1.471851821917637 \n",
            "Epoch 61, acc % = 98.95501708984375 \n",
            "\n",
            "Epoch 62, loss = 1.4717695644706676 \n",
            "Epoch 62, acc % = 98.9649658203125 \n",
            "\n",
            "Epoch 63, loss = 1.4716858772715187 \n",
            "Epoch 63, acc % = 98.95501708984375 \n",
            "\n",
            "Epoch 64, loss = 1.4716058073529772 \n",
            "Epoch 64, acc % = 98.95501708984375 \n",
            "\n",
            "Epoch 65, loss = 1.4715372825124462 \n",
            "Epoch 65, acc % = 98.97492218017578 \n",
            "\n",
            "Epoch 66, loss = 1.4714788350330037 \n",
            "Epoch 66, acc % = 98.98487091064453 \n",
            "\n",
            "Epoch 67, loss = 1.4714257679167826 \n",
            "Epoch 67, acc % = 98.99482727050781 \n",
            "\n",
            "Epoch 68, loss = 1.4713760416978483 \n",
            "Epoch 68, acc % = 98.99482727050781 \n",
            "\n",
            "Epoch 69, loss = 1.4713286616999632 \n",
            "Epoch 69, acc % = 98.99482727050781 \n",
            "\n",
            "Epoch 70, loss = 1.47128307591578 \n",
            "Epoch 70, acc % = 99.00477600097656 \n",
            "\n",
            "Epoch 71, loss = 1.471239007961978 \n",
            "Epoch 71, acc % = 99.00477600097656 \n",
            "\n",
            "Epoch 72, loss = 1.471196146527673 \n",
            "Epoch 72, acc % = 99.0246810913086 \n",
            "\n",
            "Epoch 73, loss = 1.4711543701256915 \n",
            "Epoch 73, acc % = 99.0246810913086 \n",
            "\n",
            "Epoch 74, loss = 1.471113540564373 \n",
            "Epoch 74, acc % = 99.0246810913086 \n",
            "\n",
            "Epoch 75, loss = 1.4710735037068652 \n",
            "Epoch 75, acc % = 99.0246810913086 \n",
            "\n",
            "Epoch 76, loss = 1.4710341403438787 \n",
            "Epoch 76, acc % = 99.03463745117188 \n",
            "\n",
            "Epoch 77, loss = 1.4709953373404825 \n",
            "Epoch 77, acc % = 99.04458618164062 \n",
            "\n",
            "Epoch 78, loss = 1.4709568927242498 \n",
            "Epoch 78, acc % = 99.04458618164062 \n",
            "\n",
            "Epoch 79, loss = 1.470918523278206 \n",
            "Epoch 79, acc % = 99.0545425415039 \n",
            "\n",
            "Epoch 80, loss = 1.470879939711018 \n",
            "Epoch 80, acc % = 99.06449127197266 \n",
            "\n",
            "Epoch 81, loss = 1.4708408398233401 \n",
            "Epoch 81, acc % = 99.06449127197266 \n",
            "\n",
            "Epoch 82, loss = 1.4708008895254439 \n",
            "Epoch 82, acc % = 99.06449127197266 \n",
            "\n",
            "Epoch 83, loss = 1.4707601677839923 \n",
            "Epoch 83, acc % = 99.06449127197266 \n",
            "\n",
            "Epoch 84, loss = 1.4707189730018566 \n",
            "Epoch 84, acc % = 99.07444763183594 \n",
            "\n",
            "Epoch 85, loss = 1.4706777076053013 \n",
            "Epoch 85, acc % = 99.07444763183594 \n",
            "\n",
            "Epoch 86, loss = 1.4706364209484901 \n",
            "Epoch 86, acc % = 99.09434509277344 \n",
            "\n",
            "Epoch 87, loss = 1.4705956498528743 \n",
            "Epoch 87, acc % = 99.09434509277344 \n",
            "\n",
            "Epoch 88, loss = 1.4705565605953241 \n",
            "Epoch 88, acc % = 99.09434509277344 \n",
            "\n",
            "Epoch 89, loss = 1.47052001953125 \n",
            "Epoch 89, acc % = 99.11425018310547 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}